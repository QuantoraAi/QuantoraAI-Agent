# cognition/llm/api_adapters/base.py
from __future__ import annotations
import abc
import hashlib
import logging
import time
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from loguru import logger
from prometheus_client import Counter, Histogram

# Metrics
LLM_REQUESTS = Counter(
    'llm_api_requests_total',
    'Total LLM API requests',
    ['provider', 'endpoint']
)
LLM_LATENCY = Histogram(
    'llm_api_latency_seconds',
    'API latency histogram',
    ['provider']
)

class LLMConfig(BaseModel):
    api_key: str = Field(..., min_length=32)
    endpoint: str = "https://api.example.com/v1"
    max_retries: int = 5
    timeout: float = 30.0
    rate_limit: Optional[float] = None
    model_mapping: Dict[str, str] = {}
    temperature: float = 0.7
    max_tokens: int = 2048

class LLMRequest(BaseModel):
    prompt: str
    model: str
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    stop_sequences: List[str] = []
    metadata: Dict[str, Any] = {}

class LLMResponse(BaseModel):
    text: str
    model: str
    latency: float
    tokens_used: int
    finish_reason: str
    cost_estimate: float = 0.0

class BaseLLMAdapter(abc.ABC):
    """Abstract base class for LLM provider adapters"""
    def __init__(self, config: LLMConfig):
        self.config = config
        self._last_request_time = 0.0
        self._session = self._initialize_session()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, max=10),
        retry=retry_if_exception_type((TimeoutError,))
    )
    async def generate(
        self, 
        request: LLMRequest
    ) -> LLMResponse:
        """Core generation method with enterprise-grade reliability"""
        start_time = time.monotonic()
        self._enforce_rate_limit()
        
        try:
            with LLM_LATENCY.labels(self.provider_name).time():
                response = await self._send_request(request)
                LLM_REQUESTS.labels(self.provider_name, "generate").inc()
                return self._format_response(response, start_time)
        except Exception as e:
            logger.error(f"LLM API failure: {e}")
            LLM_REQUESTS.labels(self.provider_name, "error").inc()
            raise

    @abc.abstractmethod
    async def _send_request(self, request: LLMRequest) -> Any:
        """Provider-specific implementation"""
        pass

    @abc.abstractmethod
    def _format_response(self, raw_response: Any, start_time: float) -> LLMResponse:
        """Convert vendor response to standard format"""
        pass

    @property
    @abc.abstractmethod
    def provider_name(self) -> str:
        """Name of the LLM provider"""
        pass

    def _initialize_session(self):
        """Create optimized HTTP session"""
        return aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.timeout),
            headers={
                "Authorization": f"Bearer {self.config.api_key}",
                "Content-Type": "application/json"
            }
        )

    def _enforce_rate_limit(self):
        """Precise rate limiting control"""
        if self.config.rate_limit:
            elapsed = time.monotonic() - self._last_request_time
            min_interval = 1.0 / self.config.rate_limit
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            self._last_request_time = time.monotonic()

    def _model_mapping(self, model: str) -> str:
        """Handle vendor-specific model names"""
        return self.config.model_mapping.get(model, model)

    def __del__(self):
        """Cleanup resources"""
        if self._session and not self._session.closed:
            asyncio.run(self._session.close())

# cognition/llm/api_adapters/openai_adapter.py
import openai
from typing import Optional

class OpenAIAdapter(BaseLLMAdapter):
    @property
    def provider_name(self) -> str:
        return "openai"

    async def _send_request(self, request: LLMRequest) -> Any:
        """OpenAI API implementation with automatic retry"""
        mapped_model = self._model_mapping(request.model)
        
        return await openai.Completion.acreate(
            engine=mapped_model,
            prompt=request.prompt,
            temperature=request.temperature or self.config.temperature,
            max_tokens=request.max_tokens or self.config.max_tokens,
            stop=request.stop_sequences
        )

    def _format_response(self, raw_response: Any, start_time: float) -> LLMResponse:
        return LLMResponse(
            text=raw_response.choices[0].text,
            model=raw_response.model,
            latency=time.monotonic() - start_time,
            tokens_used=raw_response.usage.total_tokens,
            finish_reason=raw_response.choices[0].finish_reason,
            cost_estimate=self._calculate_cost(raw_response)
        )

    def _calculate_cost(self, response) -> float:
        """Real-time cost estimation"""
        # Pricing per 1M tokens
        model_pricing = {
            "text-davinci-003": 0.0200,
            "code-davinci-002": 0.0200
        }
        cost = (response.usage.total_tokens / 1_000_000) * model_pricing.get(response.model, 0.02)
        return round(cost, 4)

# cognition/llm/api_adapters/anthropic_adapter.py 
import anthropic

class AnthropicAdapter(BaseLLMAdapter):
    @property
    def provider_name(self) -> str:
        return "anthropic"

    async def _send_request(self, request: LLMRequest) -> Any:
        """Anthropic Claude API implementation"""
        client = anthropic.AsyncAnthropic(api_key=self.config.api_key)
        
        return await client.completions.create(
            prompt=f"{anthropic.HUMAN_PROMPT} {request.prompt} {anthropic.AI_PROMPT}",
            model=self._model_mapping(request.model),
            max_tokens_to_sample=request.max_tokens or self.config.max_tokens,
            temperature=request.temperature or self.config.temperature,
            stop_sequences=request.stop_sequences
        )

    def _format_response(self, raw_response: Any, start_time: float) -> LLMResponse:
        return LLMResponse(
            text=raw_response.completion,
            model=raw_response.model,
            latency=time.monotonic() - start_time,
            tokens_used=raw_response.usage.output_tokens,
            finish_reason=raw_response.stop_reason,
            cost_estimate=self._calculate_cost(raw_response)
        )

    def _calculate_cost(self, response) -> float:
        model_pricing = {
            "claude-v1": 0.01102,
            "claude-instant-v1": 0.00163  
        }
        cost = (response.usage.output_tokens / 1_000_000) * model_pricing.get(response.model, 0.01)
        return round(cost, 4)

# cognition/llm/api_adapters/load_balancer.py
import random
from typing import List

class LLMLoadBalancer:
    """Enterprise-grade LLM provider load balancing"""
    def __init__(self, adapters: List[BaseLLMAdapter]):
        self.adapters = adapters
        self._health_status = {a.provider_name: 1.0 for a in adapters}
        self._performance_metrics = {}

    async def generate(
        self, 
        request: LLMRequest
    ) -> LLMResponse:
        """Intelligent routing based on multiple factors"""
        selected_adapter = self._select_adapter()
        try:
            response = await selected_adapter.generate(request)
            self._update_health(selected_adapter, success=True)
            return response
        except Exception as e:
            self._update_health(selected_adapter, success=False)
            return await self.generate(request)  # Retry with next provider

    def _select_adapter(self) -> BaseLLMAdapter:
        """Adaptive selection algorithm"""
        # Current strategy: Weighted random based on health score
        weights = [self._health_status[a.provider_name] for a in self.adapters]
        return random.choices(self.adapters, weights=weights, k=1)[0]

    def _update_health(self, adapter: BaseLLMAdapter, success: bool):
        """Dynamic health scoring"""
        current_score = self._health_status[adapter.provider_name]
        new_score = (
            min(1.0, current_score * 1.1) if success 
            else max(0.1, current_score * 0.7)
        )
        self._health_status[adapter.provider_name] = new_score

# Example Usage
async def main():
    openai_config = LLMConfig(
        api_key="sk-...",
        model_mapping={"default": "text-davinci-003"},
        rate_limit=3.0
    )
    
    anthropic_config = LLMConfig(
        api_key="sk-ant-...",
        endpoint="https://api.anthropic.com/v1",
        model_mapping={"default": "claude-v1"}
    )
    
    adapters = [
        OpenAIAdapter(openai_config),
        AnthropicAdapter(anthropic_config)
    ]
    
    lb = LLMLoadBalancer(adapters)
    
    response = await lb.generate(LLMRequest(
        prompt="Explain quantum computing in simple terms",
        model="default"
    ))
    print(f"Response: {response.text}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
